---
title: "STAT343 - Project"
author: "Hye Woong Jeon, Rohan Kapoor"
date: ""
output:
  html_document: 
    keep_md: yes
  pdf_document: default
fontsize: 10pt
abstract: ""
---

```{r include = FALSE}
library(glmnet)
library(MASS)
library(nlme)
```

In this project, we will analyze a dataset on Obsidian rocks, and try to build a working linear model for predicting the mass of a rock made of obsidian. 

## Cleaning and Exploration

```{r}
data <- read.table("data/obsidian_data.txt", header = TRUE, sep = ",")
summary(data)
```

<!-- ```{r} -->
<!-- head(data, n=10) -->
<!-- ``` -->

<!-- Data looks like it made it into R okay, so we can start analyzing it.  -->

<!-- Step 1: Data Exploration, cleaning, dealing with missing data.  -->


After importing the data, we spot some interesting features: we see a repeated ID, which suggests that an object has been logged twice. The mass data has a missing value and an extremely large value also. A few missing and a few uncertain types. An ambigious site which we should probably predict. Element Rb and Element Sr look fine, but Element Y seems to have an outlier on the high side, and Element Zr has a low side outlier. Let's look at these one by one. 


```{r}
data[which(data$ID == "288275.002bh"), ]
```
This just looks like a double-logged entry, so I will simply delete it. 

```{r}

data <- data[-33,]
#commenting out so I do not run it again, but I ran it once. 
```
Now let us look at mass. I spot a few ourliers, so I will try to look at those. 
The 160 value is an order of magntude above anything else, so I just get rid of it, since I cannot fill in the value in any way.

```{r}
data[which(data$mass >= 10), ]
#data[which(data$mass == NA), ] #no null values returned. 
```

```{r}
data <- data[-464,]
#commenting out so I do not run it again, but I ran it once. 
```
I also get rid of the NA value for mass, since I cannot impute for the regression output anyway


Now I plot the histogram of masses to see what kind of distribution it follows. 
```{r}
hist(data$mass)
```
Clearly, this does not seem normal It might be worth putting some sort of transformation onto it: probably transforming it on a log scale, or other variable. We will see about this later, but take a note of this. 

```{r}
hist(log(data$mass))
```
This looks pretty good so let's do it

```{r}
data$mass <- log(data$mass)
```


We should combine some of the type variables: blade and blades, etc. I feel pretty comfortable doing this, since all the errors seem to be for similar objects not and just logged differently by one person. 
Even if it is not perfect, it seems necessary to do since we cannot deal with that large a number of different types and simplifying to 2-3 kinds of terms helps us save degrees of freedom for other considerations later.
I first considered Retouched Blades being a different category to blades, but there are only 3 data points, which means even if they are differnet, they won't contribute much to a differnt effect, so I should just combine with Blade. Same with Used Flake to Flake.
```{r}
levels(data$type)
```

```{r}
blade_type = c(
  "Blade", 
  "Distal end of prismatic blade?",
  "Blades",
  "Retouched blades",
  "Retouched Blade",
  "blade",
  "Retouched Blades"
)
flake_type = c(
  "Flake (listed as)",
  "flake",
  "Flake",
  "Used flake",
  "Flakes"
)
core_type = c(
  "Core fragment?",
  "Core fragment",
  "Fragment (from core?)",
  "Core",
  "Cores and frags",
  "core",
  "Cores and fragments",
  "Core/Fragment"
)

blade_data = data[which(data$type %in% blade_type),]
flake_data = data[which(data$type %in% flake_type),]
core_data = data[which(data$type %in% core_type),]

blade_data$type = "Blade"
flake_data$type = "Flake"
core_data$type = "Core"

data = rbind(blade_data, flake_data, core_data)
```

```{r}
unique(data$type)
```

Also, we drop the NA entry in mass or type

```{r}
data <- data[complete.cases(data[, c('mass', 'type')]), ]
```



Now for the two site outliers. 
```{r}
data <- data[-which(data$site == "Ali Kosh/Chaga Sefid" | data$site == "Hulailan Tepe Guran"), ]
```

<!-- For the first one, we know that we just need to pick Ali Kosh/Chaga Sefid as its location, which we will do by imputing by mean. For the latter, we can either get rid of it and restrict our model to two sites, or try to learn which site looks more like Hulailan Tepe Guran. I will opt to do the latter.  -->

Now I am just going to plot the histograms of the 4 elements and see what the distribution looks like. 

```{r}
hist(data$element_Rb)
hist(data$element_Sr)
hist(data$element_Y)
hist(data$element_Zr)
```

Rb looks fine, but I think the others have outliers we can get rid of, which are probably just mis-entered data. 

```{r}
data[which(data$element_Zr<100 | data$element_Y>50 | data$element_Sr<20), ]
```
I will just delete these two 

```{r}
data <- data[-which(data$element_Zr<100 | data$element_Y>50 | data$element_Sr<20), ]
```

```{r}
summary(data)
```

The data looks clean-ish now. 

Note: note that we considered imputing by regression using a logistic regression model, but seemed too stenuous. 

```{r}
cat_covs = 3:4
cts_covs = 5:8

for (i in cat_covs) {
  data[, i] = as.factor(data[, i])
}

summary(data)
```

Looks good!

Next, we check the correlations among the continuous covariates. This can be further confirmed by plotting all the the continuous covariates against each other. Observe that all the continuous covariates are highly correlated with each other. This can be further confirmed by calculating the condition number of the design matrix (restricted to the continuous covariates) - the design matrix is clearly poorly conditioned, with a very wide range of values. Furthermore, we can regress each covariate onto the other covariates to obtain $R^2$ values. Observe that the nearly all the R-squared values are fairly high; on the other hand, `element_Y` seems to have a lower R-squared value, indicating that it is "less" collinear. We will keep this observation in mind as we build our models. In general, one of our major concerns is battling multicollinearity. 


```{r, fig.height=7}
cor(data[, cts_covs])
plot(data[, cts_covs], pch=20 , cex=1.0 , col="#69b3a2")

# condition number
cts_matrix = data.matrix(data[,cts_covs])
eigenvals = eigen(t(cts_matrix) %*% cts_matrix)
sqrt(eigenvals$val[1]/eigenvals$val)

# R-squared onto covariates
r_squared = rep(0,length(cts_covs))
for (i in 1:length(cts_covs)) {
  formula_string = paste(colnames(data)[cts_covs[i]], "~", paste(colnames(data)[cts_covs][-i], collapse="+"))
  print(formula_string)
  model = lm(formula = formula_string, data=data)
  print(summary(model))
  r_squared[i] = summary(model)$r.squared
}
r_squared
```

## Model Building

To avoid selective inference problems, we split the data into training, validation, and test sets. 

```{r}
set.seed(2)

train_idx = sample(1:(dim(data)[1]), size=0.7*dim(data)[1])
train = data[train_idx,]
not_train = data[-train_idx,]
validate_idx = sample(1:(dim(not_train)[1]), size=0.5*dim(not_train)[1])
val = not_train[-validate_idx, ]
test = not_train[validate_idx,]
```

We first fit a simple model with no interaction terms. We cycle the order of the covariates in order to ask whether the categorical covariates are significant when compared against the full model. 

```{r}
model0 = lm(formula = mass ~ element_Sr + element_Y + element_Rb + element_Zr + type + site, data=train)
anova(model0)

model0 = lm(formula = mass ~ element_Sr + element_Y + element_Rb + element_Zr + site + type, data=train)
anova(model0)
summary(model0)
```

```{r, echo=FALSE}
plot_diagnostics <- function(model, data) {
  covs = attr(model$terms , "term.labels")
  par(mfrow=c(3,2))
  for (i in 1:length(covs)) {
    plot.default(data[, covs[i]], 
                 model$residuals, 
                 main=paste(covs[i], "vs. Residuals"), 
                 xlab=covs[i],
                 type="p")
  }
}

plot_model <- function(model) {
  par(mfrow=c(2,2))
  plot(model)
}

# plot_leverage <- function(model) {
#   X = model.matrix(model)
#   lev = diag(X%*%solve(t(X)%*%X,t(X)))
#   
#   par(mfrow=c(3,2))
#   for (i in c(cat_covs, cts_covs)) {
#     plot.default(data[, i], 
#                  model$residuals, 
#                  main=paste(colnames(data)[i], "vs. Residuals"), 
#                  xlab=colnames(data)[i],
#                  type="p")
#   }
# }
```

```{r, fig.height=8}
plot_diagnostics(model0, train)
```

```{r, fig.height=7}
par(mfrow=c(2,2))
plot(model0)
```

From the F-tests, we conclude that both the categorical covariates are significant in the full model i.e. there are significant differences between sites and also between object types. Furthermore, all the element covariates are significant, with `element_Rb` having the lowest p-value. 

The diagnostic plots signify that the model is reasonably good - in particular, the linearity and normality assumptions are reasonable, save for a few outliers in the QQ plot. This suggests that interaction terms are unnecessary since no signal seems to remain. Nonetheless, we will later test for pairwise comparisons. We first some immediate problems. Firstly, the data appears to be heteroskedastic, as indicated by the sloped scale-location plot and the various diagnostic plots. The scale-location line has an upward trend, and our diagnostic plots indicate that the variance is higher at lower values of the continuous covariates (we expect less variability in the areas with few data points, and more variability in those with many data points). Additionally, the Flake type exhibits higher mass variance in comparison to the Blade type. For potentially high leverage points, there is a large value for `element_Sr` and a small value for `element_Y`. We will address each of these issues in the following order: 

1. Outliers / high leverage points / influential points
2. Variable selection / multicollinearity considerations
3. Variance stabilization using transformation and interactions terms

## Outliers / High Leverage / Influential Points

We first inspect the two points that we identified to be potentially high leverage. While the small point in `element_Y` does not have high leverage, the large point in `element_Sr` has high leverage. We save the index of this point to test for influentiality. 

```{r, fig.height=8}
# leverage
X = model.matrix(model0)
lev = diag(X%*%solve(t(X)%*%X,t(X)))

par(mfrow=c(3,1))
plot(model0$fit, model0$residuals, cex=10*lev)
plot(train[,"element_Sr"], model0$residuals, cex=10*lev)
plot(train[,"element_Y"], model0$residuals, cex=10*lev)

i1 = which.max(train[, "element_Sr"])
```

For outliers, we check the studentized residuals and apply the Bonferroni correction. Because the maximum studentized residual is less than the Bonferroni-corrected threshold, we conclude that there are no outlier points. This aligns with the diagnostic plots, as no egregious outliers are present. 

```{r}
# outliers

n=dim(train)[1]
df=summary(model0)$df[1]
print(paste0('Max studentized residual: ',max(abs(studres(model0)))))
tval = qt(1-0.05/2/n,df)
print(paste0('Bonferroni-adjusted threshold: ',tval))
```

Therefore, we have a single candidate for an influential point. We fit our original model with and without this point and observe how the fitted values change. Notice that the fitted values are almost identical, which indicates that the high leverage point is not influential enough to substantially change the model parameters. 

```{r}
model0_without_i1 = lm(formula = mass ~ element_Sr + element_Y + element_Rb + element_Zr + type + site, data=train[-i1,])

plot(model0$fitted.values[-i1], model0_without_i1$fitted.values)
abline(0,1, col=2)
cor(model0$fitted.values[-i1], model0_without_i1$fitted.values)
```

## Variable Selection and Multicollinearity

In order to reduce multicollinearity, we want to carefully select covariates to reduce the size of our model. To this end, we consider forward stepwise selection, and evaluate using the Bayesian Information Criterion (BIC) and a separate validation set. We use the `step` method. 

```{r, fig.height=8, eval=TRUE}
step(lm(mass ~ 1, data=train), direction='both', scope=formula(model0), trace=0, k=log(dim(train)[1]))
# step(lm(mass ~ type + site + element_Rb + element_Y + element_Sr + element_Zr, data=train), direction='backward', scope=formula(model0), trace=0)
```

However, the forward stepwise method selected our original model! With this, we remove covariates by hand and observe the model diagnostics of the simpler model(s). To see which covariates we should remove, we record the differences in the R-squared values of the larger and smaller models. For the model with the overall smallest change, we plot the diagnostic plots. 

```{r}
Rsq_changes = rep(summary(model0)$r.squared, length(cts_covs))

for (i in 1:length(cts_covs)) {
  formula_string = paste("mass ~ type + site + ", colnames(train)[cts_covs[i]])
  model = lm(formula = formula_string, data=train)
  Rsq_changes[i] = Rsq_changes[i] - summary(model)$r.squared
}

Rsq_changes
```

The model with the smallest change in the R-squared considers the three covariates `type`, `site`, and `element_Rb`. The diagnostics suggest that mostly everything stays the same. The R-squared values (both non-adjusted and adjusted) and the residual sum of squares have decreased and increased respectively only slightly, which indicates that our model reduction was successful. 

```{r, fig.height=8}
model1 = lm(mass ~ type + site + element_Rb, data=train)
summary(model1)
plot_diagnostics(model1, train)
plot_model(model1)
```

We check again for high leverage points and outliers, as our diagnostic plots indicate a few points that deviate from the trend quite drastically (e.g. points 103, 403). However, we do not expect these points to be influential because they lie roughly equidistant from the trend line and do not possess high leverage. Using the same quantitative method to check for outliers, no outliers could be detected. We can further check if the removal of both of these points changes the fitted values significantly. The fitted values are nearly the same regardless of the point removals. Therefore, our model looks sufficiently good. Nonetheless, we still notice a slightly decreasing trend in variance in the `element_Rb` diagnostic plot - we address this in the next section.

```{r, fig.height=8}
# leverage
X = model.matrix(model1)
lev = diag(X%*%solve(t(X)%*%X,t(X)))

par(mfrow=c(3,1))
plot(model1$fit, model1$residuals, cex=10*lev)
plot(train[,"element_Sr"], model1$residuals, cex=10*lev)
plot(train[,"element_Y"], model1$residuals, cex=10*lev)
```

```{r, fig.height=5}
# outliers
n=dim(train)[1]
df=summary(model1)$df[1]
print(paste0('Max studentized residual: ',max(abs(studres(model1)))))
tval = qt(1-0.05/2/n,df)
print(paste0('Bonferroni-adjusted threshold: ',tval))

par(mfrow=c(1,2))
plot(model1$fitted.values, model1$residuals,col="white")
text(model1$fitted.values, model1$residuals, as.character(1:dim(train)[1]))
plot(train[, "element_Rb"], model1$residuals, col="white")
text(train[, "element_Rb"], model1$residuals, as.character(1:dim(train)[1]))

```

```{r, fig.height=8}
remove_idx = c(193, 433)
model_temp = lm(formula = mass ~ element_Rb + type + site, data=train[-remove_idx,])

summary(model_temp)
```

```{r, fig.height=5}
plot(model1$fitted.values[-remove_idx], model_temp$fitted.values)
abline(0,1, col=2)
cor(model1$fitted.values[-remove_idx], model_temp$fitted.values)
```

## Heteroskedasticity 

In order to accommodate for differing variances, we consider weighted least squares. Unfortunately, weighted least squares does not 

```{r, fig.height=5}
model2 = lm(mass ~ type + site + element_Rb, data=train, weights=element_Rb)

plot(model2$fitted.values, model2$residuals / sqrt(train[, "element_Rb"]))
anova(model2)
```

On the contrary, weighted least squares appears to have overcorrected for the problem. Therefore, we do not consider the weighted least squares model. 

```{r}
# attach(train)
# model3 = gls(mass ~ type + site + element_Rb, data=train, weight = varConstPower(1, form = ~ train[,"element_Rb"]))
# fitted_sigma = (exp(model3$mod[[1]][[1]]) + (train[, "element_Rb"])^model3$mod[[1]][[2]])
# plot(train[,"element_Rb"], model3$resid /(fitted_sigma))
```

```{r}
# plot_model(model3)
```

Finally, we consider interaction terms. However, we do not suspect that 



\newpage

## Appendix

### Lasso regression

In order to reduce the effects of multicollinearity, we also tried lasso regression with various regularization parameters. We choose the best regularization parameter by doing Monte Carlo cross-validation. Monte Carlo cross-validation is a generalization of leave-one-out validation: if the dataset has size $n$, we first sample without replacement to obtain a training set of size $n_1$, then funnel the remaining $n-n_1$ points into the test set. However, lasso regression performs unsatisfactorily; there is no benefit to adding a penalty term because the validation error is monotonically increasing with increasing penalty. 

The more significant problem is that `glmnet` treats different dummy variables as different covariates altogether; for example, $\mathbb{one} \{ \text{type = Blade} \}$ is considered to be different from $\mathbb{one} \{ \text{type = FLake} \}$. This makes little sense; all the levels of a categorical predictor should be grouped together such that all or none of the levels are kept in the model (this is a similar problem to interpreting `summary()` versus `anova()`). Therefore, we elected not to use lasso for variable selection, but we have shown the results below for reference.

```{r, eval=TRUE}
mc_validation = function(trials, ratio, lambdas, data) {
  n = dim(data)[1]
  errors = rep(0, length(lambdas))
  
  for (i in 1:trials) {
    training_idx = sample(1:n, size = round(ratio * n), replace = FALSE)
    training = data[training_idx, ]
    validation = data[-training_idx, ]

    training_matrix = model.matrix(mass ~ type + site + element_Rb + element_Sr + element_Y + element_Zr, data=training)
    validation_matrix = model.matrix(mass ~ type + site + element_Rb + element_Sr + element_Y + element_Zr, data=validation)
    
    model = glmnet(x = training_matrix, y = training[, 2], lambda = lambdas)
    betahat = rbind(model$a0,as.matrix(model$beta, nrow=8, ncol=length(lambdas)))[-2,]
    predictions = validation_matrix %*% betahat
    true_value = matrix(validation[, 2], nrow = length(validation[, 2]), ncol = length(lambdas), byrow=FALSE)
    differences = predictions - true_value
    errors = errors + sqrt(colSums(differences^2))
  } 
  
  return(rev(errors / trials))
}
lambdas = seq(0, 5, 0.01)
trials = 100
training_test_ratio = 0.8

plot(lambdas, mc_validation(trials, training_test_ratio, lambdas, train), 
     type='l',
     main = "Validation Error vs. Penalty",
     xlab = "Penalty",
     ylab = "Validation Error")
```

